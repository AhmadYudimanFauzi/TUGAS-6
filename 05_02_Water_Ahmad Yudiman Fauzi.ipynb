{"cells":[{"cell_type":"markdown","metadata":{"id":"Avr_BuLrV23M"},"source":["# Assignment Ch. 5 - Transfer Learning [Case #2]\n","Startup Campus, Indonesia - `Artificial Intelligence Track`\n","* Dataset: MNIST Handwritten Digits (10 classes)\n","* Libraries: PyTorch, Torchvision, Scikit-learn\n","* Objective: Transfer Learning using CNN-based Pre-trained Models\n","\n","`PREREQUISITE` All modules (with their suitable versions) are installed properly.\n","<br>`TASK` Complete the notebook cell's code marked with <b>#TODO</b> comment.\n","<br>`TARGET PORTFOLIO` Students are able to:\n","* implement transfer learning technique using various PyTorch pre-trained models, and\n","* examine the effect of freezing some parts of the layer.\n","\n","<br>`WARNING` Do **NOT CHANGE** any codes within the User-defined Functions (UDFs) section."]},{"cell_type":"markdown","metadata":{"id":"dRm6PPoYV23R"},"source":["### Case Study Description\n","A new robotic facility located in East Kalimantan, near the Titik Nol Ibu Kota Negara (IKN) Indonesia, asks you to create a Computer Vision model for their new droid (robot) products. The company requests you to **teach the robot how to read a sequence of numbers**. You suddenly realize that the first stage is to let the robot correctly identify each individual digit (0-9). However, since the prototype announcement date was hastened, your deadline is very tight: you only have **less than 1 week** to complete the job. As a professional AI developer, you keep calm and know that you can exploit the **Transfer Learning** method to solve this problem efficiently.\n","\n","As a basic dataset in most of Computer Vision tasks, **Modified National Institute of Standards and Technology (MNIST) database** contains 10 handwritten digits. All of them are in the grayscale (1-channel). Torchvision, a sub-library of PyTorch, has dozens of pre-trained models that you can easily choose from. All of these models were originally trained on the ImageNet dataset [(ref1)](https://www.image-net.org/download.php), which contains millions of RGB (3-channel) images and 1,000 classes. For simplicity, let choose **Resnet18** [(ref2)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), **DenseNet121** [(ref3)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf), and **Vision Transformer (ViT)** [(ref4)](https://arxiv.org/pdf/2010.11929.pdf) as baseline, state-of-the-art models to test the **image classification** performance. Your complete tasks are as follows.\n","\n","1. Pick **DenseNet** as your first model to experiment with, then **change the number of neurons in the first and last layers** (since the ImageNet has 1,000 classes, while MNIST only has 10 classes; both are also come with different image size and channel).\n","2. Define **hyperparameters** and train the model (all **layers are trainable**).\n","3. Plot the model performance, for both **training** and **validation** results.\n","4. Now try to **freeze (layers are non-trainable) some parts** of layers: (1) \"denseblock1\", (2) \"denseblock1\" and \"denseblock2\". These will be two separate models.\n","5. **Retrain** each model, plot its performance, and examine the difference.\n","6. BONUS: Can you **replicate** all of the steps above with different models, i.e., **ResNet** and **ViT**?"]},{"cell_type":"markdown","metadata":{"id":"I4Hk1Q4RV23U"},"source":["[KLIK UNTUK TERJEMAHKAN TEKS](https://translate.google.com/?hl=id&ie=UTF-8&sl=en&tl=id&text=A%20new%20robotic%20facility%20located%20in%20East%20Kalimantan%2C%20near%20the%20Titik%20Nol%20Ibu%20Kota%20Negara%20(IKN)%20Indonesia%2C%20asks%20you%20to%20create%20a%20Computer%20Vision%20model%20for%20their%20new%20droid%20(robot)%20products.%20The%20company%20requests%20you%20to%20teach%20the%20robot%20how%20to%20read%20a%20sequence%20of%20numbers.%20You%20suddenly%20realize%20that%20the%20first%20stage%20is%20to%20let%20the%20robot%20correctly%20identify%20each%20individual%20digit%20(0-9).%20However%2C%20since%20the%20prototype%20announcement%20date%20was%20hastened%2C%20your%20deadline%20is%20very%20tight%3A%20you%20only%20have%20less%20than%201%20week%20to%20complete%20the%20job.%20As%20a%20professional%20AI%20developer%2C%20you%20keep%20calm%20and%20know%20that%20you%20can%20exploit%20the%20Transfer%20Learning%20method%20to%20solve%20this%20problem%20efficiently.%0A%0AAs%20a%20basic%20dataset%20in%20most%20of%20Computer%20Vision%20tasks%2C%20Modified%20National%20Institute%20of%20Standards%20and%20Technology%20(MNIST)%20database%20contains%2010%20handwritten%20digits.%20All%20of%20them%20are%20in%20the%20grayscale%20(1-channel).%20Torchvision%2C%20a%20sub-library%20of%20PyTorch%2C%20has%20dozens%20of%20pre-trained%20models%20that%20you%20can%20easily%20choose%20from.%20All%20of%20these%20models%20were%20originally%20trained%20on%20the%20ImageNet%20dataset%20(ref1)%2C%20which%20contains%20millions%20of%20RGB%20(3-channel)%20images%20and%201%2C000%20classes.%20For%20simplicity%2C%20let%20choose%20Resnet18%20(ref2)%2C%20DenseNet121%20(ref3)%2C%20and%20Vision%20Transformer%20(ViT)%20(ref4)%20as%20baseline%2C%20state-of-the-art%20models%20to%20test%20the%20image%20classification%20performance.%20Your%20complete%20tasks%20are%20as%20follows.%0A%0A1.%20Pick%20DenseNet%20as%20your%20first%20model%20to%20experiment%20with%2C%20then%20change%20the%20number%20of%20neurons%20in%20the%20first%20and%20last%20layers%20(since%20the%20ImageNet%20has%201%2C000%20classes%2C%20while%20MNIST%20only%20has%2010%20classes%3B%20both%20are%20also%20come%20with%20different%20image%20size%20and%20channel).%0A%0A2.%20Define%20hyperparameters%20and%20train%20the%20model%20(all%20layers%20are%20trainable).%0A%0A3.%20Plot%20the%20model%20performance%2C%20for%20both%20training%20and%20validation%20results.%0A%0A4.%20Now%20try%20to%20freeze%20(layers%20are%20non-trainable)%20some%20parts%20of%20layers%3A%20(1)%20%22denseblock1%22%2C%20(2)%20%22denseblock1%22%20and%20%22denseblock2%22.%20These%20will%20be%20two%20separate%20models.%0A%0A5.%20Retrain%20each%20model%2C%20plot%20its%20performance%2C%20and%20examine%20the%20difference.%0A%0A6.%20BONUS%3A%20Can%20you%20replicate%20all%20of%20the%20steps%20above%20with%20different%20models%2C%20i.e.%2C%20ResNet%20and%20ViT%3F&op=translate)"]},{"cell_type":"markdown","metadata":{"id":"AWlZH8_X1-Jc"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"EauNKxXsSznF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697288506934,"user_tz":-420,"elapsed":2193,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"e18a25cb-b0bb-41a3-cd0a-98c3c49abad3"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-5061f0ddd5d4>:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import torch, torchvision, time\n","from numpy.random import seed\n","from tqdm.autonotebook import tqdm\n","from matplotlib import pyplot as plt\n","from copy import deepcopy\n","from warnings import filterwarnings as fw; fw(\"ignore\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-JMpiYWmV23Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697288562273,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"12800a92-9a19-4a43-e162-b3ab1a5056a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":6}],"source":["torch.__version__ == \"2.0.1+cu117\""]},{"cell_type":"code","execution_count":8,"metadata":{"id":"HYpJvCkeV23a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697288602099,"user_tz":-420,"elapsed":280,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"1b73f1f0-23be-4b30-f8e0-1f1cabb78d48"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":8}],"source":["torchvision.__version__ == \"0.15.2+cu117\""]},{"cell_type":"code","source":["!pip install update torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iotOcSjvRI3C","executionInfo":{"status":"ok","timestamp":1697288384715,"user_tz":-420,"elapsed":5532,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"f67fc25c-37fe-40c7-cc37-9dd781efca9f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting update\n","  Downloading update-0.0.1-py2.py3-none-any.whl (2.9 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n","Collecting style==1.1.0 (from update)\n","  Downloading style-1.1.0-py2.py3-none-any.whl (6.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (17.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: style, update\n","Successfully installed style-1.1.0 update-0.0.1\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"-xNi2ZmbV23a","executionInfo":{"status":"ok","timestamp":1697288697467,"user_tz":-420,"elapsed":295,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["# define seeding\n","seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"BqOqMRIMV23b"},"source":["### User-defined Functions (UDFs)"]},{"cell_type":"markdown","metadata":{"id":"gwAXvOdqV23c"},"source":["- To print total model parameters"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"H4Gf78cdV23c","executionInfo":{"status":"ok","timestamp":1697288710390,"user_tz":-420,"elapsed":288,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["def check_params(model, *args, **kwargs) -> dict:\n","    return {\n","        \"total_trainable_params\" : sum(p.numel() for p in model.parameters() if p.requires_grad),\n","        \"total_nontrainable_params\" : sum(p.numel() for p in model.parameters() if not p.requires_grad)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"Us3iI6FrV23d"},"source":["- To get the pair of train and validation dataloaders"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_Aw_XFl8oCOZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697288790453,"user_tz":-420,"elapsed":1385,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"bec1270f-e86a-47af-db6f-25b858d2aa61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 35814207.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 111235715.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 24942444.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 20797520.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n","\n"]}],"source":["data_transform = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((224, 224)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((torch.tensor(33.3184)/255,), (torch.tensor(78.5675)/255,))\n","])\n","\n","train_dataset = torchvision.datasets.MNIST(root=\".\", train=True, transform=data_transform, download=True).train_data.float()\n","\n","def get_dataloaders(train_batch_size : int, val_batch_size : int, max_rows : int = 1000, *args, **kwargs) -> tuple:\n","    data_transform = torchvision.transforms.Compose([\n","        torchvision.transforms.Resize((224, 224)),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize((torch.tensor(33.3184)/255,), (torch.tensor(78.5675)/255,))\n","    ])\n","\n","    train_dataset = torchvision.datasets.MNIST(root=\".\", train=True, transform=data_transform)\n","    train_idx = torch.randperm(len(train_dataset))[:int(max_rows*.75)]\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, \\\n","                                               sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n","\n","    val_dataset = torchvision.datasets.MNIST(root=\".\", train=False, transform=data_transform)\n","    val_idx = torch.randperm(len(val_dataset))[:int(max_rows*.25)]\n","    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, \\\n","                                             sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n","\n","    return train_loader, val_loader"]},{"cell_type":"markdown","metadata":{"id":"4xHpXQlkV23e"},"source":["* To fit (training) the model"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"m05rFpG5f5yn","scrolled":true,"tags":[],"executionInfo":{"status":"ok","timestamp":1697288955658,"user_tz":-420,"elapsed":277,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["def fit(\n","    model : torchvision.models,\n","    epoch : int,\n","    train_loader : torch.utils.data.DataLoader,\n","    val_loader : torch.utils.data.DataLoader,\n","    *args, **kwargs\n",") -> dict:\n","\n","    TRAIN_LOSS, TRAIN_ACC = [], []\n","    train_batches = len(train_loader)\n","\n","    VAL_LOSS, VAL_ACC = [], []\n","    val_batches = len(val_loader)\n","\n","    # loop for every epoch (training + evaluation)\n","    start_ts = time.time()\n","    for e in range(epoch):\n","        train_losses = 0\n","        train_accuracies = 0\n","\n","        # progress bar\n","        progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=train_batches)\n","\n","        # ----------------- TRAINING  --------------------\n","        # set model to training\n","        model.train()\n","\n","        for i, data in progress:\n","            X, y = data[0].to(device), data[1].to(device)\n","\n","            # training step for single batch\n","            model.zero_grad()\n","\n","            # forward pass\n","            outputs = model(X)\n","            loss = loss_function(outputs, y)\n","\n","            # backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_losses += loss.item()\n","\n","            ps = torch.exp(outputs)\n","            top_p, top_class = ps.topk(1, dim=1)\n","            equals = top_class == y.view(*top_class.shape)\n","            train_accuracies += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","            # updating progress bar\n","            progress.set_description(\"Loss: {:.4f}\".format(train_losses/(i+1)))\n","\n","        TRAIN_ACC.append(train_accuracies/train_batches)\n","        TRAIN_LOSS.append(train_losses/train_batches)\n","\n","        # releasing unceseccary memory in GPU\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","        # ----------------- VALIDATION  -----------------\n","        val_losses = 0\n","        val_accuracies = 0\n","\n","        # set model to evaluating (testing)\n","        model.eval()\n","        with torch.no_grad():\n","            for i, data in enumerate(val_loader):\n","                X, y = data[0].to(device), data[1].to(device)\n","                outputs = model(X) # this gives the prediction from the network\n","                val_losses += loss_function(outputs, y).item()\n","\n","                ps = torch.exp(outputs)\n","                top_p, top_class = ps.topk(1, dim=1)\n","                equals = top_class == y.view(*top_class.shape)\n","                val_accuracies += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        print(\"Epoch {}/{} >> Training loss: {:.3f}, Validation loss: {:.3f}, Validation accuracy: {:.3f}\".format(\n","            e+1, epoch, train_losses/train_batches, val_losses/val_batches, val_accuracies/val_batches*100)\n","        )\n","\n","        VAL_ACC.append(val_accuracies/val_batches)\n","        VAL_LOSS.append(val_losses/val_batches)\n","\n","    tr_time = time.time()-start_ts\n","    print(\"Training time: {:.3f}s\".format(tr_time))\n","\n","    return {\n","        \"model\" : model.name,\n","        \"train_acc\" : TRAIN_ACC,\n","        \"train_loss\" : TRAIN_LOSS,\n","        \"val_acc\" : VAL_ACC,\n","        \"val_loss\" : VAL_LOSS,\n","        \"exc_time\" : tr_time\n","    }"]},{"cell_type":"markdown","metadata":{"id":"ALnyfhdrV23f"},"source":["* To visualize the model performance"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Si1GvGKHV23f","executionInfo":{"status":"ok","timestamp":1697288961030,"user_tz":-420,"elapsed":278,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["def plot_performance(dict_ : dict, *args, **kwargs) -> None:\n","    my_figure = plt.figure(figsize=(12, 4))\n","    # NOTE: figsize=(width/horizontally, height/vertically)\n","\n","    m = my_figure.add_subplot(121)\n","    plt.plot(dict_[\"train_loss\"], label=\"Train Loss\")\n","    plt.plot(dict_[\"val_loss\"], label=\"Valid. Loss\")\n","    plt.title(\"LOSS\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Score\")\n","    plt.legend(loc=\"best\")\n","\n","    n = my_figure.add_subplot(122)\n","    plt.plot(dict_[\"train_acc\"], label=\"Train Accuracy\")\n","    plt.plot(dict_[\"val_acc\"], label=\"Valid. Accuracy\")\n","    plt.title(\"ACCURACY\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Score\")\n","    plt.legend(loc=\"best\")\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bYOweBmFV23g"},"source":["### Define the model class"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"LzkK82Swc4ca","executionInfo":{"status":"ok","timestamp":1697289346166,"user_tz":-420,"elapsed":273,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["class VisionModel(torch.nn.Module):\n","    def __init__(self, model_selection : str, *args, **kwargs) -> None:\n","        super(VisionModel, self).__init__()\n","        self.model_selection = self.name = model_selection\n","        self.in_channels = 1\n","\n","        def create_conv2d(this_layer, *args, **kwargs) -> torch.nn.modules.conv.Conv2d:\n","            return torch.nn.Conv2d(\n","                in_channels=self.in_channels, out_channels=this_layer.out_channels,\n","                kernel_size=this_layer.kernel_size, stride=this_layer.stride,\n","                padding=this_layer.padding, bias=this_layer.bias\n","            )\n","\n","        if not self.model_selection.lower() in [\"resnet\", \"densenet\", \"vit\"]:\n","            raise ValueError(\"Please select the model: 'resnet', 'densenet', or 'vit'.\")\n","\n","        if self.model_selection == \"resnet\":\n","            self.model = torchvision.models.resnet18(pretrained=True)\n","            self.model.conv1 = create_conv2d(self.model.conv1) # change the input layer to take Grayscale image, instead of RGB\n","            self.model.fc = torch.nn.Linear(self.model.fc.in_features, 10) # change the output layer to output 10 classes\n","\n","        elif self.model_selection == \"densenet\":\n","            self.model = torchvision.models.densenet121(pretrained=True)\n","            self.model.features.conv0 = create_conv2d(self.model.features.conv0) # TODO: Change the DenseNet input layer stack by calling create_conv2d()\n","            self.model.classifier = torch.nn.linear(self.model.classifier.in_features, 10) # TODO: Change the DenseNet output layer with 10 classes\n","\n","        elif self.model_selection == \"vit\":\n","            self.model = torchvision.models.vit_b_16(pretrained=True)\n","            self.model.conv_proj = create_conv2d(self.model.conv_proj)# TODO: Change the ViT input layer stack by calling create_conv2d()\n","            self.model.classifier = torch.nn.Linear(self.model.heads.head.in_features, 10)# TODO: Change the ViT output layer with 10 classes\n","\n","        self.softmax = torch.nn.Softmax(dim=1)\n","\n","    def forward(self, data, *args, **kwargs) -> torchvision.models:\n","        x = self.model(data)\n","        return self.softmax(x)"]},{"cell_type":"markdown","metadata":{"id":"-5W0KXU72-PT"},"source":["### Set device to CUDA\n","On your Google Collab, click Runtime > Change Runtime Type > then select T4 GPU."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"RcXwCjR1Ylkv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697289493672,"user_tz":-420,"elapsed":275,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"3b69ebac-cba9-49f6-cbd5-fd303c35bf46"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":16}],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"bT-Q8MHzV23i"},"source":["### Define hyperparameters"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"DhYwVrbBV23i","executionInfo":{"status":"ok","timestamp":1697289530692,"user_tz":-420,"elapsed":3,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}}},"outputs":[],"source":["EPOCH = 5\n","BATCH_SIZE = 6 # TODO: Define the batch size\n","LEARNING_RATE = 1e-5 # TODO: Define the learning rate"]},{"cell_type":"markdown","metadata":{"id":"i72AP_NXV23j"},"source":["### Define the model"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"pEr4mIDdV23j","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"error","timestamp":1697289714444,"user_tz":-420,"elapsed":719,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"e58e434f-7461-489f-f59b-bfcc8b9f05f7"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-c78de45da10f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Pass the string \"resnet\" for ResNet18, \"densenet\" for DenseNet121, and \"vit\" for Vision Transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"densenet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-e6c328fb992e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_selection, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: Change the DenseNet input layer stack by calling create_conv2d()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: Change the DenseNet output layer with 10 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'linear'"]}],"source":["# TODO: Pass the string \"resnet\" for ResNet18, \"densenet\" for DenseNet121, and \"vit\" for Vision Transformer\n","model = VisionModel(\"densenet\").to(device)\n","check_params(model)"]},{"cell_type":"code","execution_count":25,"metadata":{"scrolled":true,"tags":[],"id":"IAEbBYGvV23j","colab":{"base_uri":"https://localhost:8080/","height":179},"executionInfo":{"status":"error","timestamp":1697289744191,"user_tz":-420,"elapsed":265,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"81277dde-5a5f-49ef-ac64-47fbad9ca4fb"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-1f8a688cae5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"Q-n1bdNDV23k"},"source":["### WILL BE USED LATER: Freeze some layers"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"orYYwn0ZV23k","colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"status":"error","timestamp":1697289749626,"user_tz":-420,"elapsed":295,"user":{"displayName":"Ahmad Yudiman Fauzi","userId":"15768261908996318521"}},"outputId":"6fb113cc-ec37-464e-d9f1-fe78f19c9383"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-a2ec67258773>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_freeze_block1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_freeze_block1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"denseblock1\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_freeze_block1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["model_freeze_block1 = deepcopy(model)\n","for name, param in model_freeze_block1.named_parameters():\n","    if param.requires_grad and \"denseblock1\" in name:\n","        param.requires_grad = False\n","check_params(model_freeze_block1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjiUG3w1V23k"},"outputs":[],"source":["model_freeze_block12 = deepcopy(model)\n","for name, param in model_freeze_block12.named_parameters():\n","    if param.requires_grad and any([x in name for x in [\"denseblock1\", \"denseblock2\"]]):\n","        param.requires_grad = False\n","check_params(model_freeze_block12)"]},{"cell_type":"markdown","metadata":{"id":"W8CslUMA1FZD"},"source":["### Get train and validation dataloaders"]},{"cell_type":"markdown","metadata":{"id":"Xn3YgFa_V23l"},"source":["To speedup the training time, we will only use 1,000 (of 60,000) images from MNIST."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnEsZPvpV23l"},"outputs":[],"source":["train_loader, val_loader = get_dataloaders(BATCH_SIZE, BATCH_SIZE)\n","len(train_loader), len(val_loader)"]},{"cell_type":"markdown","metadata":{"id":"EpktUiY9V23m"},"source":["### Set loss function and model optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APMQehx_V23m"},"outputs":[],"source":["loss_function = torch.nn.CrossEntropyLoss() # Define the loss function (for multi-classification)\n","\n","trainable_model_params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adam(trainable_model_params, lr=LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{"id":"0F6Qym3NV23p"},"source":["### Start the model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPAy2FwgV23p"},"outputs":[],"source":["# TODO: Specify variables for your model, number of epochs, train data loader, and validation data loader\n","results = fit(\n","    model =model,\n","    epoch =EPOCH,\n","    train_loader =train_loader,\n","    val_loader =val_loader\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"tags":[],"id":"iCDJXY5mV23q"},"outputs":[],"source":["results"]},{"cell_type":"markdown","metadata":{"id":"iw48D8F2V23q"},"source":["### Plot the model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMvs9dtHV23r"},"outputs":[],"source":["plot_performance(results)"]},{"cell_type":"markdown","metadata":{"id":"CjBoorynV23r"},"source":["### NEXT ROUND: Retrain the model with frozen layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iH50oGbV23s"},"outputs":[],"source":["FROZEN_RESULTS = []\n","for idx, m in enumerate([model_freeze_block1, model_freeze_block12]):\n","    print(\"id: {}\".format(idx))\n","    trainable_model_params = [p for p in m.parameters() if p.requires_grad]\n","    optimizer = torch.optim.Adam(trainable_model_params, lr=LEARNING_RATE)\n","\n","    new_results = fit(model=m, epoch=EPOCH, train_loader=train_loader, val_loader=val_loader)\n","    FROZEN_RESULTS.append(new_results)"]},{"cell_type":"markdown","metadata":{"id":"ua9XK664V23s"},"source":["### Examine the difference in both accuracy and loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhOvNRhnV23t"},"outputs":[],"source":["plot_performance(FROZEN_RESULTS[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpU86rI7V23t"},"outputs":[],"source":["plot_performance(FROZEN_RESULTS[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFJg2iI3V23t"},"outputs":[],"source":["# QUESTIONS\n","# TODO: With the same 5 epochs in training, why Transfer Learning with frozen layers are worse in the final accuracy?"]},{"cell_type":"markdown","metadata":{"id":"vIO43pQeV23u"},"source":["[ ANSWER HERE ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIb644GNV23u"},"outputs":[],"source":["# QUESTIONS\n","# TODO: Why the more layers are frozen, the lower the accuracy of the model in the early (the 1st) epoch?"]},{"cell_type":"markdown","metadata":{"id":"vtv1O48sV23u"},"source":["[ ANSWER HERE ]"]},{"cell_type":"markdown","metadata":{"id":"oFlZouUDV23v"},"source":["### Examine the difference in the execution time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9w_0AAkV23v"},"outputs":[],"source":["print(\"When all layers were TRAINABLE: {:.3f}s.\".format(results[\"exc_time\"]))\n","print(\"Only 'denseblock1' was FROZEN: {:.3f}s.\".format(FROZEN_RESULTS[0][\"exc_time\"]))\n","print(\"Only 'denseblock1' and 'denseblock2' wwere FROZEN: {:.3f}s.\".format(FROZEN_RESULTS[1][\"exc_time\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xbm_3QaBV23v"},"outputs":[],"source":["# QUESTIONS\n","# TODO: Why the more layers are frozen, the faster the training-validation time?"]},{"cell_type":"markdown","metadata":{"id":"qzkxY5yMV23v"},"source":["[ ANSWER HERE ]"]},{"cell_type":"markdown","metadata":{"id":"Kd8Td1YsV23w"},"source":["### Scoring\n","Total `#TODO` = 12\n","<br>Checklist:\n","\n","- [ ] Change the DenseNet input layer stack by calling create_conv2d()\n","- [ ] Change the DenseNet output layer with 10 classes\n","- [ ] Change the ViT input layer stack by calling create_conv2d()\n","- [ ] Change the ViT output layer with 10 classes\n","- [ ] Define the batch size\n","- [ ] Define the learning rate\n","- [ ] Define the loss function (for multi-classification)\n","- [ ] Pass the string \"resnet\" for ResNet18, \"densenet\" for DenseNet121, and \"vit\" for Vision Transformer\n","- [ ] Specify variables for your model, number of epochs, train data loader, and validation data loader\n","- [ ] QUESTION: With the same 5 epochs in training, why Transfer Learning with frozen layers are worse in the final accuracy?\n","- [ ] QUESTION: Why the more layers are frozen, the lower the accuracy of the model in the early (the 1st) epoch?\n","- [ ] QUESTION: Why the more layers are frozen, the faster the training-validation time?"]},{"cell_type":"markdown","metadata":{"id":"nu54eCVrV23w"},"source":["### Additional readings\n","* ResNet: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n","* DenseNet: https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\n","* Vision Transformer (ViT): https://arxiv.org/pdf/2010.11929.pdf\n","* MNIST Classification w/ PyTorch (Beginner): https://www.kaggle.com/code/amsharma7/mnist-pytorch-for-beginners-detailed-desc"]},{"cell_type":"markdown","metadata":{"id":"8lozabSPV23w"},"source":["### Copyright © 2023 Startup Campus, Indonesia\n","* You may **NOT** use this file except there is written permission from PT. Kampus Merdeka Belajar (Startup Campus).\n","* Please address your questions to mentors."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"12xP8825vrpTNRe_DTscoAMLesHcM-gEI","timestamp":1697290233554},{"file_id":"https://github.com/kjamithash/Pytorch_DeepLearning_Experiments/blob/master/FashionMNIST_ResNet_TransferLearning.ipynb","timestamp":1681550748297}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}